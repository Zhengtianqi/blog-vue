<template><div><h1 id="一、spark的核心组件是" tabindex="-1"><a class="header-anchor" href="#一、spark的核心组件是" aria-hidden="true">#</a> 一、Spark的核心组件是：</h1>
<p>​				集群资源管理服务（Cluster Manager）</p>
<p>​				运行作业任务的节点（WorkerNode），</p>
<p>​				每个应用的任务控制节点 Driver 和 每个机器节点上有具有任务的执行进程（Executor）</p>
<figure><img src="/assets/images/Spark.png" alt="image-20191218134210879" tabindex="0" loading="lazy"><figcaption>image-20191218134210879</figcaption></figure>
<p>说明：</p>
<figure><img src="/assets/images/spark-all.png" alt="image-20191218140600902" tabindex="0" loading="lazy"><figcaption>image-20191218140600902</figcaption></figure>
<h1 id="二、关键概念" tabindex="-1"><a class="header-anchor" href="#二、关键概念" aria-hidden="true">#</a> 二、关键概念</h1>
<p>（1）RDD</p>
<p>​		Spark 的核心概念是弹性分布式数据集。RDD 是一个只读且不可变的分布式对象集合，创建、转化即调用 RDD 操作者一系列过程贯穿于 Spark 大数据处理的始终。</p>
<p>（2）DAG</p>
<p>​		Spark使用有向无环图进行任务调度。</p>
<p>（3）Spark SQL</p>
<p>​		用于结构化数据的计算。</p>
<p>（4）DataFrame</p>
<p>​		分布式的、按照名名列的形式组织的数据集合。</p>
<p>（5）SQLContext</p>
<p>​		Spark SQL 提供 SQLContext 封装 Spark 中的所有关系型功能，可以用前面提到的SparkContext创建SQLContext。</p>
<p>（6）JDBC数据源</p>
<p>三、Spark 和 HDFS 的配合关系</p>
<p>​		<img src="/assets/images/spark+hdfs.png" alt="image-20191218141731121" loading="lazy"></p>
<ul>
<li>（1）读取文件的详细步骤：</li>
<li>SparkScheduler 与 HDFS 交互获取 File A 的文件信息。</li>
<li>HDFS返回该文件具体的 Block 信息</li>
<li>SparkScheduler 根据具体的 Block 数据量，决定一个并行度，创建多个 Task 去读取这些文件Block</li>
<li>Executor 端执行 Task 并读取具体的 Block，作为 RDD（弹性分部数据集）的一部分</li>
<li>（2）HDFS文件写入的详细步骤：</li>
<li>SparkScheduler 创建要写入文件的目录</li>
<li>根据 RDD 分区分块情况，计算写出数据的 Task 数，并下发这些任务到 Executor</li>
<li>Executor 执行这些 Task，将具体 RDD 的数据写入到第一步创建的目录下</li>
</ul>
</div></template>


